<html>

<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Cloris (Xiaoqi) Li </title>
  <meta content="Cloris (Xiaoqi) Li" name="keywords" />
  <style media="screen" type="text/css">
    html,
    body,
    div,
    span,
    applet,
    object,
    iframe,
    h1,
    h2,
    h3,
    h4,
    h5,
    h6,
    p,
    blockquote,
    pre,
    a,
    abbr,
    acronym,
    address,
    big,
    cite,
    code,
    del,
    dfn,
    em,
    font,
    img,
    ins,
    kbd,
    q,
    s,
    samp,
    small,
    strike,
    strong,
    sub,
    tt,
    var,
    dl,
    dt,
    dd,
    ol,
    ul,
    li,
    fieldset,
    form,
    label,
    legend,
    table,
    caption,
    tbody,
    tfoot,
    thead,
    tr,
    th,
    td {
      border: 0pt none;
      font-family: Arial, Helvetica, sans-serif;
      font-size: 100%;
      font-style: inherit;
      font-weight: inherit;
      margin: 0pt;
      outline-color: invert;
      outline-style: none;
      outline-width: 0pt;
      padding: 0pt;
      vertical-align: baseline;
    }

    a {
      color: #043d98;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    a.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    b.paper {
      font-weight: bold;
      font-size: 12pt;
    }

    * {
      margin: 0pt;
      padding: 0pt;
    }

    body {
      position: relative;
      margin: 2em auto 2em auto;
      width: 825px;
      font-family: Open Sans Light, Helvetica, sans-serif;
      font-size: 14px;
      background: #F4F6F6;
    }

    h2 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 15pt;
      font-weight: 700;
      padding-bottom: 8px;
      padding-top: 8px;
      background-color: rgb(194, 228, 250);
      border-radius: 15px;
      padding-left: 8px;
      margin-bottom: 8px;
      margin-top: 8px;
    }

    h3 {
      font-family: Lato, Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700;
      margin-top: 10px;
      padding-left: 8px;
    }

    strong {
      /* font-family: Lato, Verdana, Helvetica, sans-serif; */
      /* font-size: 13px; */
      font-weight: bold;
    }

    ul {
      /* list-style: circle; */
      list-style: disc;
    }

    img {
      border: none;
    }

    li {
      padding-bottom: 0.5em;
      margin-left: 1.4em;
    }

    alert {
      font-family: Arial, Helvetica, sans-serif;
      /*font-size: 13px;*/
      /* font-weight: bold; */
      color: #FF0000;
    }

    em,
    i {
      font-style: italic;
    }

    div.section {
      clear: both;
      /* margin-bottom: 1.2em; */
      /* margin-top: 3em; */
      /* background: #F4F6F6; */
      background: #FFFFFF;
    }

    div.spanner {
      clear: both;
    }

    div.paper {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em .6em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 140%;
    }

    div.paper2 {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em 0.6em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 140%;
    }

    /* div.paper:hover { */
    /* background: #FFFDEE; */
    /* background-color: #242d36 ; */
    /* } */

    div.paper2:hover {
      background: #FFFDEE;
      /* background-color: #242d36 ; */
    }

    div.bio {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.7em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.55em .8em 0.6em .7em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 135%;
    }

    div.res {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.4em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.65em .8em 0.15em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 130%;
    }
    div.title{
      margin-bottom: 20px;
      border: 1px solid #ddd;
    }
    div.award {
      clear: both;
      margin-top: 0.4em;
      margin-bottom: 0.4em;
      border: 0px solid #ddd;
      background: #fff;
      padding: 0.65em .8em 0.15em .8em;
      border-top-right-radius: 10px;
      border-top-left-radius: 10px;
      border-bottom-left-radius: 10px;
      border-bottom-right-radius: 10px;
      line-height: 130%;
    }

    div.paper div {
      padding-left: 230px;
    }

    img.paper {
      margin-bottom: 0.5em;
      float: left;
      width: 200px;
    }

    span.blurb {
      font-style: italic;
      display: block;
      margin-top: 0.75em;
      margin-bottom: 0.5em;
    }

    pre,
    code {
      font-family: Open Sans Light, Helvetica, sans-serif;
      font-size: 13px;
      margin: 1em 0;
      padding: 0;
    }

    .bot {
      font-size: 14%;
    }

    .ptypej {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #5cb85c;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    .ptypec {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #428bca;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    .ptypep {
      display: inline;
      padding: .0em .2em .05em;
      font-size: 85%;
      font-weight: bold;
      line-height: 1;
      background-color: #6B6B6B;
      color: #FFFFFF;
      text-align: center;
      white-space: nowrap;
      vertical-align: baseline;
      margin-right: 6px;
    }

    /* navigation */
    #nav {
      /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
      /* Corbel, Arial, Helvetica, sans-serif; */
      /* font-family: Georgia, Helvetica, sans-serif; */
      /* position: fixed; */
      /* top: 50px; */
      /* left: 860px; */
      /* margin-left: 810px;     1060 */
      /* width: 92px; */
      /* font-size: 14px; */
      list-style-type: none;
      margin: 0;
      padding: 0;
      overflow: hidden;
      background-color: rgb(194, 228, 250);
      position: fixed;
      top: 0;
      width: 100%;
      left: 0;
      height: 50px;
      font-size: 16px;
      /* float: right */
    }

    #nav li {
      margin-bottom: 1px;
      float: right;
      margin-top: 5px;
      font-size: large;
    }

    ol {
      list-style: none;
    }

    #nav a {
      /* display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none; */
      display: block;
      color: black;
      text-align: center;
      padding: 11px 25px;
      text-decoration: none;
    }

    #nav a:hover {
      color: #ffde00;
      /* background-color: #242d36 ; */
    }
  </style>

  <!-- <script type="text/javascript" src="./files/hidebib.js"></script> -->

  <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
    type="text/css" />
</head>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
</script>

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body style="background-color:#FFFFFF">

  <ol id="nav">
    <img title="Dobby baby" style="float: right; height: 40px;border-radius: 10px;padding-top: 5px;padding-bottom: 5px;padding-right: 10px;" src="icon2.jpg"
        alt="cloris li" />
    <li><a href="mailto:xl3062@columbia.edu" title="Contact">Contact</a></li>
    <li><a href="#pub" title="Papers">Publications</a></li>
    <!-- <li><a href="#news" title="News">News</a></li> -->
    <li><a href="#home" title="Home">Home</a></li>
  </ol>

  <!-- Home section -->
  <a name="home"><br /><br /><br /></a>
  <div 
    style="margin-bottom: 8px; margin-top: 8px;  height:350px; padding: 1em; ">
    <div style="margin: 0px auto; width: 100%;">
      <img title="Cloris (Xiaoqi) Li (李晓琦)" style="float: right;  height: 200px;border-radius: 50%;" src="cloris_crop.jpg"
        alt="cloris li" />
        <img title="Dobby baby" style="float: right; height: 200px;border-radius: 50%;" src="dobby2.jpg"
        alt="cloris li" />
      <div style="padding-left: 2em; vertical-align: top; height: 100px;"><span
          style="line-height: 200%; font-size: 24pt;">Cloris (Xiaoqi) Li </span><br />
          <span style="font-size: 16pt;line-height: 34px;"><strong>P.h.D candidate </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.pku.edu.cn/"> Peking University</a></span><br />
        <span style="font-size: 16pt;line-height: 34px;"><strong>Master degree </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.columbia.edu/">Columbia University in the city of New York</a></span><br />  
          <span style="font-size: 16pt;line-height: 34px;"><strong>Bachelor degree </strong></span><br />
          <span style="font-size: 12pt;line-height: 34px;"><a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a></span><br />  
          <span style="font-size: 12pt;line-height: 34px;"><strong>Email: </strong><a href="mailto:">xl3062@columbia.edu</a></span><br />
        <!-- <span style="font-size: 12pt;line-height: 34px;"><strong>Address: </strong><a>J12/ 1 Cleveland St, Darlington, NSW 2008, Australia</a></span><br /> -->
      </div>
    </div>
  </div>
  <!-- <div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;"> -->

  <div style="margin-top: 0.2em;">
    <div class="section">
      <h2>About Me (<a href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ&hl=en&oi=ao">Google Scholar</a>)</h2>
      <h3> Personal Info</h3>
      <div class="bio">
        I am now a P.h.D candidate at Peking University major in robotic and computer vision, supervised by Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a> .
        Before that, I recieved my master degree at columbia university, concrete jungle where dreams are made of. I majored in machine learning and computer vision tracks.
        I recieved my B.Eng degree at Beijing University of Posts and Telecommunications.
        I have the cutiest baby Dobby, the best golden retriver in the world. Dobby was born at Apr 19, 2022 and is named after the elf in Harry Potter. 
      </div>
      <h3> Internship</h3>
      <div class="bio"> 
        From 08/2021 to 03/2023, I was a research intern at OPPO Research Institute, supervised by <a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ&hl=en">Yandong Guo</a>.<br>
        From 06/2020 to 05/2021, I was a research intern at Intel Label China, supervised by Ming Lu. <br>       
      </div>

    </div>
  </div>

  <div style="clear: both;">
    <div class="section">
      <a name="interests"></a>
      <h2>Research Interests</h2>
      <div class="bio">
        <ul>
          My research mainly focuses on Robot Manipulation and Computer Vision.
        </ul>

      </div>
    </div>
  </div>


  <a name="pub"></a>
  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Publications</h2>
      <div class="paper" id="corl2025"><img class="paper" src="papers/corl2025.png" />
        <div>
          <p><a><b>3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation</b></a><br />
            <u><b>Xiaoqi Li, Liang Heng, Jiaming Liu, Yan Shen, Chenyang Gu, Zhuoyang Liu, Hao Chen, Nuowei Han, Renrui Zhang, Hao Tang, Shanghang Zhang, Hao Dong
          </p>Conference on Computer Vision and Pattern Recognition (CVPR), 2025<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="iros2026-2"><img class="paper" src="papers/iros2026-2.png" />
        <div>
          <p><a><b>SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping</b></a><br />
            <u><b>Mingxu Zhang*, <u><b>Xiaoqi Li*</b></u>, Jiahui Xu, Kaichen Zhou, Hojin Bae, Yan Shen, Chuyan Xiong, Hao Dong
          </p>Conference on Computer Vision and Pattern Recognition (CVPR), 2025<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="iros2026-1"><img class="paper" src="papers/iros2026-1.png" />
        <div>
          <p><a><b>RwoR: Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot</b></a><br />
            <u><b>Liang Heng*, <u><b>Xiaoqi Li*</b></u>, Shangqing Mao, Jiaming Liu, Ruolin Liu, Jingli Wei, Yu-Kai Wang, Jia Yueru, Chenyang Gu, Rui Zhao, Shanghang Zhang, Hao Dong
          </p>Conference on Computer Vision and Pattern Recognition (CVPR), 2025<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="cvpr2025"><img class="paper" src="papers/cvpr2025.png" />
        <div>
          <p><a><b>CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation</b></a><br />
            <u><b>Xiaoqi Li</b></u>, Lingyun Xu, Mingxu Zhang, Jiaming Liu, Yan Shen, Iaroslav Ponomarenko, Jiahui Xu, Liang Heng, Siyuan Huang, Shanghang Zhang, Hao Dong
          </p>Conference on Computer Vision and Pattern Recognition (CVPR), 2025<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="icra2025-2"><img class="paper" src="papers/icra2025-3dwg.png" />
        <div>
          <p><a><b>3D Weakly Supervised Visual Grounding at Category and Instance Levels</b></a><br />
            <u><b>Xiaoqi Li</b></u>, Jiaming Liu, Yandong Guo, Hao Dong, Yang Liu
          </p>International Conference on Robotics and Automation (ICRA), 2025<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="icra2025-1"><img class="paper" src="papers/icra2025-spatialbot.png" />
        <div>
          <p><a><b>Spatialbot: Precise spatial understanding with vision language models</b></a><br />
            Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan,  <u><b>Xiaoqi Li</b></u>, Wankou Yang, Hao Dong, Bo Zhao
          </p>International Conference on Robotics and Automation (ICRA), 2025<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2406.13642'>[PDF]</a>&nbsp;
            <a href='https://github.com/BAAI-DCAI/SpatialBot'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ral2024"><img class="paper" src="papers/ral2024.png" />
        <div>
          <p><a><b>Naturalvlm: Leveraging fine-grained natural language for affordance-guided visual manipulation</b></a><br />
            Ran Xu, Yan Shen,  <u><b>Xiaoqi Li</b></u>, Ruihai Wu, Hao Dong
          </p>Robotics and Automation Letters (RAL), 2024<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://ieeexplore.ieee.org/abstract/document/10711281'>[PDF]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="corl2024"><img class="paper" src="papers/corl2024.png" />
        <div>
          <p><a><b>Autonomous interactive correction MLLM for robust robotic manipulation</b></a><br />
            Chuyan Xiong*, Chengyu Shen*,  <u><b>Xiaoqi Li*</b></u>, Kaichen Zhou, Jiaming Liu, Ruiping Wang, Hao Dong
          </p>Conference on Robot Learning (CoRL), 2024<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openreview.net/pdf?id=S8jQtafbT3'>[PDF]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ICRA2024"><img class="paper" src="papers/2024ManipVQA-min.jpg" />
        <div>
          <p><a><b>ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models</b></a><br />
            Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, <u><b>Xiaoqi Li</b></u>, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong
          </p>International Conference on Intelligent Robots and Systems (IROS), 2024, <b><font color="DarkRed">ORAL</font></b> <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2403.11289'>[PDF]</a>&nbsp;
            <a href='https://github.com/SiyuanHuang95/ManipVQA'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ICRA2024"><img class="paper" src="papers/2024CVPR-ManipLLM-min.jpg" />
        <div>
          <p><a><b>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</b></a><br />
            <u><b>Xiaoqi Li</b></u>, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, Hao Dong
          </p>Conference on Computer Vision and Pattern Recognition (CVPR), 2024<br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2312.16217'>[PDF]</a>&nbsp;
            <a href='https://github.com/clorislili/ManipLLM'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ICRA2024"><img class="paper" src="papers/2024ICRA-rgbmanip-min.jpg" />
        <div>
          <p><a><b>RGBManip: Monocular Image-based Robotic Manipulation through Active Object Pose Estimation</b></a><br />
            Boshi An, Yiran Geng, Kai Chen, <u><b>Xiaoqi Li</b></u>, Qi Dou, Hao Dong
          </p>International Conference on Robotics and Automation (ICRA), 2024  <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2310.03478'>[PDF]</a>&nbsp;
            <a href='https://github.com/hyperplane-lab/RGBManip'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>


      <div class="paper" id="ICRA2024"><img class="paper" src="papers/2024ICRA-DiscussNav-min.jpg" />
        <div>
          <p><a><b>Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions</b></a><br />
            Yuxing Long, <u><b>Xiaoqi Li</b></u>, Wenzhe Cai, Hao Dong
          </p>International Conference on Robotics and Automation (ICRA), 2024  <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/pdf/2309.11382'>[PDF]</a>&nbsp;
            <a href='https://github.com/LYX0501/DiscussNav'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>


      <div class="paper" id="NeurIPS2023"><img class="paper" src="papers/2023neurips-ddn-min.jpg" />
        <div>
          <p><a><b>Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation</b></a><br />
            Hongcheng Wang, Andy Guan Hong Chen, <u><b>Xiaoqi Li</b></u>, Mingdong Wu, Hao Dong
          </p>Neural Information Processing Systems (NeurIPS),  2023  <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2309.08138'>[PDF]</a>&nbsp;
            <a href='https://github.com/whcpumpkin/Demand-driven-navigation'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ECCV2022"><img class="paper" src="papers/ECCV2022/eccv2022.PNG" />
        <div>
          <p><a><b>Efficient Meta-Tuning for Content-aware Neural Video Delivery</b></a><br />
            <u><b>Xiaoqi Li</b></u>, Jiaming Liu, Shizun Wang, Cheng Lyu, Ming Lu, Yurong Chen, Anbang Yao, Yandong Guo, Shanghang Zhang
          </p>European Conference on Computer Vision (ECCV), 2022 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2207.09691'>[PDF]</a>&nbsp;
            <a href='https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>

      <div class="paper" id="ECCV2022"><img class="paper" src="papers/ECCV2022/eccv2022O.PNG" />
        <div>
          <p><a><b>Adaptive Patch Exiting for Scalable Single Image Super-Resolution</b></a><br />
            Shizun Wang, Jiaming Liu, Kaixin Chen, <u><b>Xiaoqi Li</b></u>, Ming Lu, Yandong Guo
          </p>European Conference on Computer Vision (ECCV), 2022, <b><font color="DarkRed">ORAL</font></b>. <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://arxiv.org/abs/2203.11589'>[PDF]</a>&nbsp;
            <a href='https://pythondig.com/r/adaptive-patch-exiting-for-scalable-single-image-superresolution-eccv-oral'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>
  

       <!-- ICCV2021 -->
       <div class="paper" id="ICCV2021"><img class="paper" src="papers/ICCV2021/ICCV2021.png" />
        <div>
          <p><a><b>Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation</b></a><br />
           Jiaming Liu, Ming Lu, Kaixin Chen,  <u><b>Xiaoqi Li</b></u>, Shizun Wang, Zhaoqing Wang, Enhua Wu, Yurong Chen, Chuang Zhang, Ming Wu
          </p>International Conference on Computer Vision (ICCV), 2021 <br /><p>
            <!-- <b><font color="Green">[Poster]</font></b>&nbsp; -->
            <a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Overfitting_the_Data_Compact_Neural_Video_Delivery_via_Content-Aware_Feature_ICCV_2021_paper.pdf'>[PDF]</a>&nbsp;
            <a href='https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021'>[Code]</a>&nbsp;
            <!-- <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>[Slides]</a>&nbsp; -->
        </div>
        <div class="spanner"></div>
      </div>



     

    

      <div style="clear:both;">
        <p align="right">
          <font size="5">Last Updated on Aug, 2025</a></font>
        </p>
      </div>

      <!-- <hr>
      <div id="clustrmaps-widget"></div>
      <script type="text/javascript" id="clustrmaps"
        src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=3SbcvEtmc7QY9AEk0QS8aX5NjWpb6rfNMsOGTchiXzs"></script> -->

</body>

</html>